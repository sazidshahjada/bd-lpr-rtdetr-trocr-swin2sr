{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = \"/home/iot/bd-lpr-rtdetr-trocr-swin2sr\"\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5777a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import roboflow\n",
    "import supervision\n",
    "import transformers\n",
    "import pytorch_lightning\n",
    "print(\n",
    "    \"roboflow:\", roboflow.__version__,\n",
    "    \"; supervision:\", supervision.__version__,\n",
    "    \"; transformers:\", transformers.__version__,\n",
    "    \"; pytorch_lightning:\", pytorch_lightning.__version__\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56173aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrImageProcessor, RTDetrV2ForObjectDetection\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT = 'PekingU/rtdetr_v2_r101vd'\n",
    "CONFIDENCE_TRESHOLD = 0.5\n",
    "IOU_TRESHOLD = 0.8\n",
    "image_processor = RTDetrImageProcessor.from_pretrained(CHECKPOINT)\n",
    "model = RTDetrV2ForObjectDetection.from_pretrained(CHECKPOINT)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"85ptyZqJterXZHoJSHfp\")\n",
    "project = rf.workspace(\"roboflow-universe-projects\").project(\"license-plate-recognition-rxg4e\")\n",
    "version = project.version(11)\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
    "TRAIN_DIRECTORY = os.path.join(dataset.location, \"train\")\n",
    "VAL_DIRECTORY = os.path.join(dataset.location, \"valid\")\n",
    "TEST_DIRECTORY = os.path.join(dataset.location, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03343eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, image_directory_path: str, image_processor, train: bool = True):\n",
    "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3012aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = CocoDetection(image_directory_path=TRAIN_DIRECTORY, image_processor=image_processor, train=True)\n",
    "VAL_DATASET = CocoDetection(image_directory_path=VAL_DIRECTORY, image_processor=image_processor, train=False)\n",
    "TEST_DATASET = CocoDetection(image_directory_path=TEST_DIRECTORY, image_processor=image_processor, train=False)\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e055339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
    "image_id = random.choice(image_ids)\n",
    "print('Image #{}'.format(image_id))\n",
    "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
    "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
    "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
    "image = cv2.imread(image_path)\n",
    "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
    "categories = TRAIN_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k, v in categories.items()}\n",
    "labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
    "\n",
    "%matplotlib inline\n",
    "sv.show_frame_in_notebook(image, (16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class RTDETR(pl.LightningModule):\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        self.model = RTDetrV2ForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=CHECKPOINT,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation/loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad], \"lr\": self.lr_backbone},\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TRAIN_DATALOADER\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VAL_DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'lightning_logs/version_6/checkpoints/epoch=41-step=4662.ckpt'\n",
    "model = RTDETR.load_from_checkpoint(checkpoint_path, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "batch = next(iter(TRAIN_DATALOADER))\n",
    "batch = {\n",
    "    key: value.to(DEVICE) if isinstance(value, torch.Tensor) else value \n",
    "    for key, value in batch.items()\n",
    "}\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# %cd {HOME}\n",
    "# MAX_EPOCHS = 10\n",
    "# model.train()\n",
    "# trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "image_ids = TEST_DATASET.coco.getImgIds()\n",
    "image_id = random.choice(image_ids)\n",
    "print('Image #{}'.format(image_id))\n",
    "image = TEST_DATASET.coco.loadImgs(image_id)[0]\n",
    "annotations = TEST_DATASET.coco.imgToAnns[image_id]\n",
    "image_path = os.path.join(TEST_DATASET.root, image['file_name'])\n",
    "image = cv2.imread(image_path)\n",
    "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
    "labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n",
    "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
    "print('ground truth')\n",
    "%matplotlib inline\n",
    "sv.show_frame_in_notebook(frame, (16, 16))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors='pt')\n",
    "    inputs = image_processor.pad(inputs['pixel_values'], return_tensors='pt')\n",
    "    inputs.to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
    "    results = image_processor.post_process_object_detection(outputs=outputs, threshold=CONFIDENCE_TRESHOLD, target_sizes=target_sizes)[0]\n",
    "detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=0.5)\n",
    "labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
    "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
    "print('detections')\n",
    "%matplotlib inline\n",
    "sv.show_frame_in_notebook(frame, (16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "def prepare_for_coco_detection(predictions):\n",
    "    coco_results = []\n",
    "    for original_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = convert_to_xywh(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        coco_results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                }\n",
    "                for k, box in enumerate(boxes)\n",
    "            ]\n",
    "        )\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "evaluator = CocoEvaluator(coco_gt=TEST_DATASET.coco, iou_types=[\"bbox\"])\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(TEST_DATALOADER)):\n",
    "    pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "    labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes)\n",
    "\n",
    "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    predictions = prepare_for_coco_detection(predictions)\n",
    "    evaluator.update(predictions)\n",
    "\n",
    "evaluator.synchronize_between_processes()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e467481",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(HOME, 'rtdetr')\n",
    "model.model.save_pretrained(MODEL_PATH)\n",
    "image_processor.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrImageProcessor, RTDetrV2ForObjectDetection\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = RTDetrV2ForObjectDetection.from_pretrained(\"rtdetr\")\n",
    "processor = RTDetrImageProcessor.from_pretrained(\"rtdetr\")\n",
    "model.config.id2label = {0: \"Unknown\", 1: \"License Plate\"}\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import supervision as sv\n",
    "\n",
    "# Open video\n",
    "video_path = \"License Plate Detection Test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video\")\n",
    "    exit()\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"Video: {total_frames} frames, {fps} FPS, {width}x{height}\")\n",
    "\n",
    "# Set up output video\n",
    "output_path = \"rtdetr_output_video_3.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize annotator\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "# Process frames\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing\")\n",
    "for _ in range(total_frames):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Skipped frame {_}\")\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Preprocess and infer\n",
    "        inputs = processor(images=frame, return_tensors='pt')\n",
    "        inputs = processor.pad(inputs['pixel_values'], return_tensors='pt')\n",
    "        inputs.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        target_sizes = torch.tensor([frame.shape[:2]]).to(device)\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs=outputs, threshold=0.5, target_sizes=target_sizes\n",
    "        )[0]\n",
    "\n",
    "    # Annotate\n",
    "    detections = sv.Detections.from_transformers(transformers_results=results)\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=frame, detections=detections, labels=[f\"{model.config.id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
    "    )\n",
    "\n",
    "    # Write frame\n",
    "    out.write(annotated_frame)\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Output saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdd469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
